package start.case6TridentTest.operations.n05aggregate;

import org.apache.storm.Config;
import org.apache.storm.LocalCluster;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.trident.TridentTopology;
import org.apache.storm.trident.testing.FixedBatchSpout;
import org.apache.storm.tuple.Fields;
import start.case6TridentTest.operations.Datas;
import start.case6TridentTest.operations.n01filter.Filter;
import start.case6TridentTest.operations.n02function.Function;

public class Demo1ReducerAggre {

    public static StormTopology buildTopology() {

        FixedBatchSpout spout = new Datas().getSpout();
        spout.setCycle(true);

        TridentTopology topology = new TridentTopology();
        topology.newStream("spout", spout)
                .each(new Fields("date", "amt", "city", "product"), new Function.OperFunction(), new Fields("_date"))// 此时的Tuple有"date", "amt", "city", "product", "_date"5列数据
                .project(new Fields("_date", "amt"))// 投影操作：只保留 _date,amt 两个字段的数据，相当于select选取字段
                .aggregate(new Fields("amt"), new Aggregate.ReducerAggre(), new Fields("_amt"))// BaseAggregator, CombinerAggregator, ReducerAggregator
                .each(new Fields("_amt"), new Filter.PrintFilter())// 打印分区数，分区索引，tuple值
        ;
        return topology.build();
    }

    public static void main(String[] args) {
        Config conf = new Config();
        conf.setMaxSpoutPending(20);
        conf.setDebug(false);
        if (args.length == 0) {
            LocalCluster cluster = new LocalCluster();
            cluster.submitTopology("wordCounter", conf, buildTopology());
        }
    }
}
